2020-10-21 12:44:50,572 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting NameNode
STARTUP_MSG:   host = master/172.18.0.9
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.3
STARTUP_MSG:   classpath = /usr/local/hadoop/etc/hadoop/:/usr/local/hadoop/share/hadoop/common/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/common/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-digester-1.8.jar:/usr/local/hadoop/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-framework-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-configuration-1.6.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-httpclient-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/common/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/common/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-collections-3.2.2.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-annotations-2.7.3.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-auth-2.7.3.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/httpcore-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/httpclient-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/usr/local/hadoop/share/hadoop/common/lib/jets3t-0.9.0.jar:/usr/local/hadoop/share/hadoop/common/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-net-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/usr/local/hadoop/share/hadoop/common/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/common/lib/jsch-0.1.42.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.3-tests.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.3.jar:/usr/local/hadoop/share/hadoop/common/hadoop-nfs-2.7.3.jar:/usr/local/hadoop/share/hadoop/hdfs:/usr/local/hadoop/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.3.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.3.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.3-tests.jar:/usr/local/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.3.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.3.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.7.3.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.3.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.3.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.7.3.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.7.3.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.3.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.3.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.3.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.7.3.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.7.3.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.3-tests.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.3.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r baa91f7c6bc9cb92be5982de4719c1c8af91ccff; compiled by 'root' on 2016-08-18T01:41Z
STARTUP_MSG:   java = 1.8.0_151
************************************************************/
2020-10-21 12:44:50,590 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]
2020-10-21 12:44:50,595 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: createNameNode []
2020-10-21 12:44:50,923 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2020-10-21 12:44:51,009 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2020-10-21 12:44:51,010 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: NameNode metrics system started
2020-10-21 12:44:51,012 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: fs.defaultFS is hdfs://master:9000
2020-10-21 12:44:51,013 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Clients are to use master:9000 to access this namenode/service.
2020-10-21 12:44:51,331 INFO org.apache.hadoop.hdfs.DFSUtil: Starting Web-server for hdfs at: http://master:50070
2020-10-21 12:44:51,408 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2020-10-21 12:44:51,418 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2020-10-21 12:44:51,427 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.namenode is not defined
2020-10-21 12:44:51,436 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2020-10-21 12:44:51,442 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hdfs
2020-10-21 12:44:51,442 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2020-10-21 12:44:51,443 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2020-10-21 12:44:51,636 INFO org.apache.hadoop.http.HttpServer2: Added filter 'org.apache.hadoop.hdfs.web.AuthFilter' (class=org.apache.hadoop.hdfs.web.AuthFilter)
2020-10-21 12:44:51,639 INFO org.apache.hadoop.http.HttpServer2: addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.namenode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2020-10-21 12:44:51,659 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50070
2020-10-21 12:44:51,660 INFO org.mortbay.log: jetty-6.1.26
2020-10-21 12:44:51,826 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@master:50070
2020-10-21 12:44:51,859 WARN org.apache.hadoop.hdfs.server.common.Util: Path /data/dfs/name should be specified as a URI in configuration files. Please update hdfs configuration.
2020-10-21 12:44:51,861 WARN org.apache.hadoop.hdfs.server.common.Util: Path /data/dfs/name should be specified as a URI in configuration files. Please update hdfs configuration.
2020-10-21 12:44:51,862 WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Only one image storage directory (dfs.namenode.name.dir) configured. Beware of data loss due to lack of redundant storage directories!
2020-10-21 12:44:51,862 WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Only one namespace edits storage directory (dfs.namenode.edits.dir) configured. Beware of data loss due to lack of redundant storage directories!
2020-10-21 12:44:51,868 WARN org.apache.hadoop.hdfs.server.common.Util: Path /data/dfs/name should be specified as a URI in configuration files. Please update hdfs configuration.
2020-10-21 12:44:51,869 WARN org.apache.hadoop.hdfs.server.common.Util: Path /data/dfs/name should be specified as a URI in configuration files. Please update hdfs configuration.
2020-10-21 12:44:51,908 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: No KeyProvider found.
2020-10-21 12:44:51,908 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsLock is fair:true
2020-10-21 12:44:51,960 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.block.invalidate.limit=1000
2020-10-21 12:44:51,961 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true
2020-10-21 12:44:51,962 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2020-10-21 12:44:51,963 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: The block deletion will start around 2020 Oct 21 12:44:51
2020-10-21 12:44:51,965 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlocksMap
2020-10-21 12:44:51,965 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2020-10-21 12:44:51,967 INFO org.apache.hadoop.util.GSet: 2.0% max memory 889 MB = 17.8 MB
2020-10-21 12:44:51,968 INFO org.apache.hadoop.util.GSet: capacity      = 2^21 = 2097152 entries
2020-10-21 12:44:51,975 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.block.access.token.enable=false
2020-10-21 12:44:51,975 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: defaultReplication         = 1
2020-10-21 12:44:51,976 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplication             = 512
2020-10-21 12:44:51,976 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: minReplication             = 1
2020-10-21 12:44:51,976 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplicationStreams      = 2
2020-10-21 12:44:51,977 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: replicationRecheckInterval = 3000
2020-10-21 12:44:51,977 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: encryptDataTransfer        = false
2020-10-21 12:44:51,978 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxNumBlocksToLog          = 1000
2020-10-21 12:44:51,986 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsOwner             = root (auth:SIMPLE)
2020-10-21 12:44:51,986 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: supergroup          = supergroup
2020-10-21 12:44:51,987 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: isPermissionEnabled = false
2020-10-21 12:44:51,987 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: HA Enabled: false
2020-10-21 12:44:51,990 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Append Enabled: true
2020-10-21 12:44:52,053 INFO org.apache.hadoop.util.GSet: Computing capacity for map INodeMap
2020-10-21 12:44:52,054 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2020-10-21 12:44:52,054 INFO org.apache.hadoop.util.GSet: 1.0% max memory 889 MB = 8.9 MB
2020-10-21 12:44:52,054 INFO org.apache.hadoop.util.GSet: capacity      = 2^20 = 1048576 entries
2020-10-21 12:44:52,074 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: ACLs enabled? false
2020-10-21 12:44:52,074 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: XAttrs enabled? true
2020-10-21 12:44:52,075 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: Maximum size of an xattr: 16384
2020-10-21 12:44:52,075 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Caching file names occuring more than 10 times
2020-10-21 12:44:52,081 INFO org.apache.hadoop.util.GSet: Computing capacity for map cachedBlocks
2020-10-21 12:44:52,081 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2020-10-21 12:44:52,082 INFO org.apache.hadoop.util.GSet: 0.25% max memory 889 MB = 2.2 MB
2020-10-21 12:44:52,082 INFO org.apache.hadoop.util.GSet: capacity      = 2^18 = 262144 entries
2020-10-21 12:44:52,084 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2020-10-21 12:44:52,084 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.min.datanodes = 0
2020-10-21 12:44:52,084 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.extension     = 30000
2020-10-21 12:44:52,088 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10
2020-10-21 12:44:52,088 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10
2020-10-21 12:44:52,088 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25
2020-10-21 12:44:52,090 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Retry cache on namenode is enabled
2020-10-21 12:44:52,091 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
2020-10-21 12:44:52,093 INFO org.apache.hadoop.util.GSet: Computing capacity for map NameNodeRetryCache
2020-10-21 12:44:52,093 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2020-10-21 12:44:52,094 INFO org.apache.hadoop.util.GSet: 0.029999999329447746% max memory 889 MB = 273.1 KB
2020-10-21 12:44:52,094 INFO org.apache.hadoop.util.GSet: capacity      = 2^15 = 32768 entries
2020-10-21 12:44:52,107 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /data/dfs/name/in_use.lock acquired by nodename 231@master
2020-10-21 12:44:52,178 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Recovering unfinalized segments in /data/dfs/name/current
2020-10-21 12:44:52,182 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: No edit log streams selected.
2020-10-21 12:44:52,183 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Planning to load image: FSImageFile(file=/data/dfs/name/current/fsimage_0000000000000000000, cpktTxId=0000000000000000000)
2020-10-21 12:44:52,241 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatPBINode: Loading 1 INodes.
2020-10-21 12:44:52,267 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: Loaded FSImage in 0 seconds.
2020-10-21 12:44:52,268 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Loaded image for txid 0 from /data/dfs/name/current/fsimage_0000000000000000000
2020-10-21 12:44:52,280 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Need to save fs image? false (staleImage=false, haEnabled=false, isRollingUpgrade=false)
2020-10-21 12:44:52,282 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 1
2020-10-21 12:44:52,395 INFO org.apache.hadoop.hdfs.server.namenode.NameCache: initialized with 0 entries 0 lookups
2020-10-21 12:44:52,395 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Finished loading FSImage in 297 msecs
2020-10-21 12:44:52,705 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: RPC server is binding to master:9000
2020-10-21 12:44:52,717 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2020-10-21 12:44:52,733 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 9000
2020-10-21 12:44:52,774 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Registered FSNamesystemState MBean
2020-10-21 12:44:52,776 WARN org.apache.hadoop.hdfs.server.common.Util: Path /data/dfs/name should be specified as a URI in configuration files. Please update hdfs configuration.
2020-10-21 12:44:52,795 INFO org.apache.hadoop.hdfs.server.namenode.LeaseManager: Number of blocks under construction: 0
2020-10-21 12:44:52,796 INFO org.apache.hadoop.hdfs.server.namenode.LeaseManager: Number of blocks under construction: 0
2020-10-21 12:44:52,796 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: initializing replication queues
2020-10-21 12:44:52,798 INFO org.apache.hadoop.hdfs.StateChange: STATE* Leaving safe mode after 0 secs
2020-10-21 12:44:52,799 INFO org.apache.hadoop.hdfs.StateChange: STATE* Network topology has 0 racks and 0 datanodes
2020-10-21 12:44:52,799 INFO org.apache.hadoop.hdfs.StateChange: STATE* UnderReplicatedBlocks has 0 blocks
2020-10-21 12:44:52,815 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2020-10-21 12:44:52,819 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Total number of blocks            = 0
2020-10-21 12:44:52,820 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of invalid blocks          = 0
2020-10-21 12:44:52,820 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of under-replicated blocks = 0
2020-10-21 12:44:52,821 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of  over-replicated blocks = 0
2020-10-21 12:44:52,821 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of blocks being written    = 0
2020-10-21 12:44:52,822 INFO org.apache.hadoop.hdfs.StateChange: STATE* Replication Queue initialization scan for invalid, over- and under-replicated blocks completed in 24 msec
2020-10-21 12:44:52,900 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2020-10-21 12:44:52,900 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 9000: starting
2020-10-21 12:44:52,908 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: NameNode RPC up at: master/172.18.0.9:9000
2020-10-21 12:44:52,909 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Starting services required for active state
2020-10-21 12:44:52,915 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Starting CacheReplicationMonitor with interval 30000 milliseconds
2020-10-21 12:44:55,950 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* registerDatanode: from DatanodeRegistration(172.18.0.9:50010, datanodeUuid=6158f795-6f5a-4077-b880-90e3ab329bf6, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-3e8c1414-db15-4700-b0c2-df0389673a7b;nsid=2041325608;c=0) storage 6158f795-6f5a-4077-b880-90e3ab329bf6
2020-10-21 12:44:55,951 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2020-10-21 12:44:55,952 INFO org.apache.hadoop.net.NetworkTopology: Adding a new node: /default-rack/172.18.0.9:50010
2020-10-21 12:44:56,015 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2020-10-21 12:44:56,015 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Adding new storage ID DS-f81e3095-598a-4077-8723-10720198009d for DN 172.18.0.9:50010
2020-10-21 12:44:56,068 INFO BlockStateChange: BLOCK* processReport: from storage DS-f81e3095-598a-4077-8723-10720198009d node DatanodeRegistration(172.18.0.9:50010, datanodeUuid=6158f795-6f5a-4077-b880-90e3ab329bf6, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-3e8c1414-db15-4700-b0c2-df0389673a7b;nsid=2041325608;c=0), blocks: 0, hasStaleStorage: false, processing time: 2 msecs
2020-10-21 12:45:06,548 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741825_1001{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-f81e3095-598a-4077-8723-10720198009d:NORMAL:172.18.0.9:50010|RBW]]} for /tmp/hadoop-root/yarn/system/rmstore/FSRMStateRoot/RMVersionNode.tmp
2020-10-21 12:45:07,004 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741825_1001{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-f81e3095-598a-4077-8723-10720198009d:NORMAL:172.18.0.9:50010|RBW]]} is not COMPLETE (ucState = COMMITTED, replication# = 0 <  minimum = 1) in file /tmp/hadoop-root/yarn/system/rmstore/FSRMStateRoot/RMVersionNode.tmp
2020-10-21 12:45:07,004 INFO org.apache.hadoop.hdfs.server.namenode.EditLogFileOutputStream: Nothing to flush
2020-10-21 12:45:07,031 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.9:50010 is added to blk_1073741825_1001{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-f81e3095-598a-4077-8723-10720198009d:NORMAL:172.18.0.9:50010|RBW]]} size 4
2020-10-21 12:45:07,420 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-root/yarn/system/rmstore/FSRMStateRoot/RMVersionNode.tmp is closed by DFSClient_NONMAPREDUCE_-1644123298_1
2020-10-21 12:45:07,465 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741826_1002{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-f81e3095-598a-4077-8723-10720198009d:NORMAL:172.18.0.9:50010|RBW]]} for /tmp/hadoop-root/yarn/system/rmstore/FSRMStateRoot/EpochNode.tmp
2020-10-21 12:45:07,504 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.9:50010 is added to blk_1073741826_1002{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-f81e3095-598a-4077-8723-10720198009d:NORMAL:172.18.0.9:50010|RBW]]} size 0
2020-10-21 12:45:07,515 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-root/yarn/system/rmstore/FSRMStateRoot/EpochNode.tmp is closed by DFSClient_NONMAPREDUCE_-1644123298_1
2020-10-21 12:45:07,592 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741827_1003{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-f81e3095-598a-4077-8723-10720198009d:NORMAL:172.18.0.9:50010|RBW]]} for /tmp/hadoop-root/yarn/system/rmstore/FSRMStateRoot/AMRMTokenSecretManagerRoot/AMRMTokenSecretManagerNode.tmp
2020-10-21 12:45:07,639 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.9:50010 is added to blk_1073741827_1003{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-f81e3095-598a-4077-8723-10720198009d:NORMAL:172.18.0.9:50010|RBW]]} size 0
2020-10-21 12:45:07,644 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-root/yarn/system/rmstore/FSRMStateRoot/AMRMTokenSecretManagerRoot/AMRMTokenSecretManagerNode.tmp is closed by DFSClient_NONMAPREDUCE_-1644123298_1
2020-10-21 12:45:07,660 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741828_1004{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-f81e3095-598a-4077-8723-10720198009d:NORMAL:172.18.0.9:50010|RBW]]} for /tmp/hadoop-root/yarn/system/rmstore/FSRMStateRoot/RMDTSecretManagerRoot/DelegationKey_1.tmp
2020-10-21 12:45:07,681 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741828_1004{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-f81e3095-598a-4077-8723-10720198009d:NORMAL:172.18.0.9:50010|RBW]]} is not COMPLETE (ucState = COMMITTED, replication# = 0 <  minimum = 1) in file /tmp/hadoop-root/yarn/system/rmstore/FSRMStateRoot/RMDTSecretManagerRoot/DelegationKey_1.tmp
2020-10-21 12:45:07,682 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.9:50010 is added to blk_1073741828_1004{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-f81e3095-598a-4077-8723-10720198009d:NORMAL:172.18.0.9:50010|RBW]]} size 17
2020-10-21 12:45:08,093 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-root/yarn/system/rmstore/FSRMStateRoot/RMDTSecretManagerRoot/DelegationKey_1.tmp is closed by DFSClient_NONMAPREDUCE_-1644123298_1
2020-10-21 12:45:08,131 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741829_1005{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-f81e3095-598a-4077-8723-10720198009d:NORMAL:172.18.0.9:50010|RBW]]} for /tmp/hadoop-root/yarn/system/rmstore/FSRMStateRoot/RMDTSecretManagerRoot/DelegationKey_2.tmp
2020-10-21 12:45:08,171 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.18.0.9:50010 is added to blk_1073741829_1005{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-f81e3095-598a-4077-8723-10720198009d:NORMAL:172.18.0.9:50010|RBW]]} size 0
2020-10-21 12:45:08,179 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-root/yarn/system/rmstore/FSRMStateRoot/RMDTSecretManagerRoot/DelegationKey_2.tmp is closed by DFSClient_NONMAPREDUCE_-1644123298_1
2020-10-21 12:46:11,017 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting NameNode
STARTUP_MSG:   host = master/172.21.0.9
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.3
STARTUP_MSG:   classpath = /usr/local/hadoop/etc/hadoop/:/usr/local/hadoop/share/hadoop/common/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/common/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-digester-1.8.jar:/usr/local/hadoop/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-framework-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-configuration-1.6.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-httpclient-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/common/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/common/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-collections-3.2.2.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-annotations-2.7.3.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-auth-2.7.3.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/httpcore-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/httpclient-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/usr/local/hadoop/share/hadoop/common/lib/jets3t-0.9.0.jar:/usr/local/hadoop/share/hadoop/common/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-net-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/usr/local/hadoop/share/hadoop/common/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/common/lib/jsch-0.1.42.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.3-tests.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.3.jar:/usr/local/hadoop/share/hadoop/common/hadoop-nfs-2.7.3.jar:/usr/local/hadoop/share/hadoop/hdfs:/usr/local/hadoop/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.3.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.3.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.3-tests.jar:/usr/local/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.3.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.3.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.7.3.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.3.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.3.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.7.3.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.7.3.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.3.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.3.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.3.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.7.3.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.7.3.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.3-tests.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.3.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r baa91f7c6bc9cb92be5982de4719c1c8af91ccff; compiled by 'root' on 2016-08-18T01:41Z
STARTUP_MSG:   java = 1.8.0_151
************************************************************/
2020-10-21 12:46:11,039 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]
2020-10-21 12:46:11,045 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: createNameNode []
2020-10-21 12:46:11,371 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2020-10-21 12:46:11,504 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2020-10-21 12:46:11,504 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: NameNode metrics system started
2020-10-21 12:46:11,509 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: fs.defaultFS is hdfs://master:9000
2020-10-21 12:46:11,510 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Clients are to use master:9000 to access this namenode/service.
2020-10-21 12:46:11,713 INFO org.apache.hadoop.hdfs.DFSUtil: Starting Web-server for hdfs at: http://master:50070
2020-10-21 12:46:11,779 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2020-10-21 12:46:11,790 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2020-10-21 12:46:11,799 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.namenode is not defined
2020-10-21 12:46:11,807 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2020-10-21 12:46:11,810 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hdfs
2020-10-21 12:46:11,811 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2020-10-21 12:46:11,812 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2020-10-21 12:46:11,968 INFO org.apache.hadoop.http.HttpServer2: Added filter 'org.apache.hadoop.hdfs.web.AuthFilter' (class=org.apache.hadoop.hdfs.web.AuthFilter)
2020-10-21 12:46:11,971 INFO org.apache.hadoop.http.HttpServer2: addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.namenode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2020-10-21 12:46:11,992 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50070
2020-10-21 12:46:11,993 INFO org.mortbay.log: jetty-6.1.26
2020-10-21 12:46:12,168 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@master:50070
2020-10-21 12:46:12,200 WARN org.apache.hadoop.hdfs.server.common.Util: Path /data/dfs/name should be specified as a URI in configuration files. Please update hdfs configuration.
2020-10-21 12:46:12,202 WARN org.apache.hadoop.hdfs.server.common.Util: Path /data/dfs/name should be specified as a URI in configuration files. Please update hdfs configuration.
2020-10-21 12:46:12,208 WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Only one image storage directory (dfs.namenode.name.dir) configured. Beware of data loss due to lack of redundant storage directories!
2020-10-21 12:46:12,208 WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Only one namespace edits storage directory (dfs.namenode.edits.dir) configured. Beware of data loss due to lack of redundant storage directories!
2020-10-21 12:46:12,215 WARN org.apache.hadoop.hdfs.server.common.Util: Path /data/dfs/name should be specified as a URI in configuration files. Please update hdfs configuration.
2020-10-21 12:46:12,216 WARN org.apache.hadoop.hdfs.server.common.Util: Path /data/dfs/name should be specified as a URI in configuration files. Please update hdfs configuration.
2020-10-21 12:46:12,249 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: No KeyProvider found.
2020-10-21 12:46:12,249 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsLock is fair:true
2020-10-21 12:46:12,302 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.block.invalidate.limit=1000
2020-10-21 12:46:12,303 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true
2020-10-21 12:46:12,305 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2020-10-21 12:46:12,306 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: The block deletion will start around 2020 Oct 21 12:46:12
2020-10-21 12:46:12,309 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlocksMap
2020-10-21 12:46:12,310 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2020-10-21 12:46:12,312 INFO org.apache.hadoop.util.GSet: 2.0% max memory 889 MB = 17.8 MB
2020-10-21 12:46:12,313 INFO org.apache.hadoop.util.GSet: capacity      = 2^21 = 2097152 entries
2020-10-21 12:46:12,322 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.block.access.token.enable=false
2020-10-21 12:46:12,323 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: defaultReplication         = 1
2020-10-21 12:46:12,323 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplication             = 512
2020-10-21 12:46:12,323 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: minReplication             = 1
2020-10-21 12:46:12,324 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplicationStreams      = 2
2020-10-21 12:46:12,324 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: replicationRecheckInterval = 3000
2020-10-21 12:46:12,325 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: encryptDataTransfer        = false
2020-10-21 12:46:12,325 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxNumBlocksToLog          = 1000
2020-10-21 12:46:12,335 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsOwner             = root (auth:SIMPLE)
2020-10-21 12:46:12,336 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: supergroup          = supergroup
2020-10-21 12:46:12,336 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: isPermissionEnabled = false
2020-10-21 12:46:12,337 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: HA Enabled: false
2020-10-21 12:46:12,339 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Append Enabled: true
2020-10-21 12:46:12,410 INFO org.apache.hadoop.util.GSet: Computing capacity for map INodeMap
2020-10-21 12:46:12,410 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2020-10-21 12:46:12,411 INFO org.apache.hadoop.util.GSet: 1.0% max memory 889 MB = 8.9 MB
2020-10-21 12:46:12,411 INFO org.apache.hadoop.util.GSet: capacity      = 2^20 = 1048576 entries
2020-10-21 12:46:12,412 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: ACLs enabled? false
2020-10-21 12:46:12,413 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: XAttrs enabled? true
2020-10-21 12:46:12,413 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: Maximum size of an xattr: 16384
2020-10-21 12:46:12,413 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Caching file names occuring more than 10 times
2020-10-21 12:46:12,422 INFO org.apache.hadoop.util.GSet: Computing capacity for map cachedBlocks
2020-10-21 12:46:12,422 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2020-10-21 12:46:12,423 INFO org.apache.hadoop.util.GSet: 0.25% max memory 889 MB = 2.2 MB
2020-10-21 12:46:12,423 INFO org.apache.hadoop.util.GSet: capacity      = 2^18 = 262144 entries
2020-10-21 12:46:12,425 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2020-10-21 12:46:12,426 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.min.datanodes = 0
2020-10-21 12:46:12,426 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.extension     = 30000
2020-10-21 12:46:12,431 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10
2020-10-21 12:46:12,431 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10
2020-10-21 12:46:12,432 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25
2020-10-21 12:46:12,434 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Retry cache on namenode is enabled
2020-10-21 12:46:12,434 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
2020-10-21 12:46:12,437 INFO org.apache.hadoop.util.GSet: Computing capacity for map NameNodeRetryCache
2020-10-21 12:46:12,437 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2020-10-21 12:46:12,438 INFO org.apache.hadoop.util.GSet: 0.029999999329447746% max memory 889 MB = 273.1 KB
2020-10-21 12:46:12,438 INFO org.apache.hadoop.util.GSet: capacity      = 2^15 = 32768 entries
2020-10-21 12:46:12,454 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /data/dfs/name/in_use.lock acquired by nodename 235@master
2020-10-21 12:46:12,536 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Recovering unfinalized segments in /data/dfs/name/current
2020-10-21 12:46:12,712 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Finalizing edits file /data/dfs/name/current/edits_inprogress_0000000000000000001 -> /data/dfs/name/current/edits_0000000000000000001-0000000000000000040
2020-10-21 12:46:12,722 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Planning to load image: FSImageFile(file=/data/dfs/name/current/fsimage_0000000000000000000, cpktTxId=0000000000000000000)
2020-10-21 12:46:12,772 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatPBINode: Loading 1 INodes.
2020-10-21 12:46:12,879 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: Loaded FSImage in 0 seconds.
2020-10-21 12:46:12,881 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Loaded image for txid 0 from /data/dfs/name/current/fsimage_0000000000000000000
2020-10-21 12:46:12,882 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@e98770d expecting start txid #1
2020-10-21 12:46:12,883 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file /data/dfs/name/current/edits_0000000000000000001-0000000000000000040
2020-10-21 12:46:12,886 INFO org.apache.hadoop.hdfs.server.namenode.EditLogInputStream: Fast-forwarding stream '/data/dfs/name/current/edits_0000000000000000001-0000000000000000040' to transaction ID 1
2020-10-21 12:46:12,936 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file /data/dfs/name/current/edits_0000000000000000001-0000000000000000040 of size 1048576 edits # 40 loaded in 0 seconds
2020-10-21 12:46:12,938 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Need to save fs image? false (staleImage=false, haEnabled=false, isRollingUpgrade=false)
2020-10-21 12:46:12,940 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 41
2020-10-21 12:46:13,042 INFO org.apache.hadoop.hdfs.server.namenode.NameCache: initialized with 0 entries 0 lookups
2020-10-21 12:46:13,043 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Finished loading FSImage in 600 msecs
2020-10-21 12:46:13,232 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: RPC server is binding to master:9000
2020-10-21 12:46:13,241 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2020-10-21 12:46:13,254 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 9000
2020-10-21 12:46:13,285 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Registered FSNamesystemState MBean
2020-10-21 12:46:13,286 WARN org.apache.hadoop.hdfs.server.common.Util: Path /data/dfs/name should be specified as a URI in configuration files. Please update hdfs configuration.
2020-10-21 12:46:13,298 INFO org.apache.hadoop.hdfs.server.namenode.LeaseManager: Number of blocks under construction: 0
2020-10-21 12:46:13,298 INFO org.apache.hadoop.hdfs.server.namenode.LeaseManager: Number of blocks under construction: 0
2020-10-21 12:46:13,299 INFO org.apache.hadoop.hdfs.StateChange: STATE* Safe mode ON. 
The reported blocks 0 needs additional 5 blocks to reach the threshold 0.9990 of total blocks 5.
The number of live datanodes 0 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached.
2020-10-21 12:46:13,306 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2020-10-21 12:46:13,336 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2020-10-21 12:46:13,338 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 9000: starting
2020-10-21 12:46:13,357 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: NameNode RPC up at: master/172.21.0.9:9000
2020-10-21 12:46:13,357 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Starting services required for active state
2020-10-21 12:46:13,363 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Starting CacheReplicationMonitor with interval 30000 milliseconds
2020-10-21 12:46:16,489 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* registerDatanode: from DatanodeRegistration(172.21.0.9:50010, datanodeUuid=6158f795-6f5a-4077-b880-90e3ab329bf6, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-3e8c1414-db15-4700-b0c2-df0389673a7b;nsid=2041325608;c=0) storage 6158f795-6f5a-4077-b880-90e3ab329bf6
2020-10-21 12:46:16,489 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2020-10-21 12:46:16,490 INFO org.apache.hadoop.net.NetworkTopology: Adding a new node: /default-rack/172.21.0.9:50010
2020-10-21 12:46:16,561 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2020-10-21 12:46:16,561 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Adding new storage ID DS-f81e3095-598a-4077-8723-10720198009d for DN 172.21.0.9:50010
2020-10-21 12:46:16,597 INFO org.apache.hadoop.hdfs.StateChange: STATE* Safe mode extension entered. 
The reported blocks 4 has reached the threshold 0.9990 of total blocks 5. The number of live datanodes 1 has reached the minimum number 0. In safe mode extension. Safe mode will be turned off automatically in 29 seconds.
2020-10-21 12:46:16,597 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: initializing replication queues
2020-10-21 12:46:16,600 INFO BlockStateChange: BLOCK* processReport: from storage DS-f81e3095-598a-4077-8723-10720198009d node DatanodeRegistration(172.21.0.9:50010, datanodeUuid=6158f795-6f5a-4077-b880-90e3ab329bf6, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-3e8c1414-db15-4700-b0c2-df0389673a7b;nsid=2041325608;c=0), blocks: 5, hasStaleStorage: false, processing time: 8 msecs
2020-10-21 12:46:16,602 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Total number of blocks            = 5
2020-10-21 12:46:16,602 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of invalid blocks          = 0
2020-10-21 12:46:16,603 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of under-replicated blocks = 0
2020-10-21 12:46:16,603 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of  over-replicated blocks = 0
2020-10-21 12:46:16,604 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of blocks being written    = 0
2020-10-21 12:46:16,604 INFO org.apache.hadoop.hdfs.StateChange: STATE* Replication Queue initialization scan for invalid, over- and under-replicated blocks completed in 5 msec
2020-10-21 12:46:26,669 INFO org.apache.hadoop.ipc.Server: IPC Server handler 4 on 9000, call org.apache.hadoop.hdfs.protocol.ClientProtocol.mkdirs from 172.21.0.9:53982 Call#0 Retry#0: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot create directory /tmp/hadoop-root/yarn/system/rmstore/FSRMStateRoot/RMDTSecretManagerRoot. Name node is in safe mode.
The reported blocks 5 has reached the threshold 0.9990 of total blocks 5. The number of live datanodes 1 has reached the minimum number 0. In safe mode extension. Safe mode will be turned off automatically in 19 seconds.
2020-10-21 12:46:28,198 INFO org.apache.hadoop.ipc.Server: IPC Server handler 7 on 9000, call org.apache.hadoop.hdfs.protocol.ClientProtocol.mkdirs from 172.21.0.9:53982 Call#0 Retry#1: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot create directory /tmp/hadoop-root/yarn/system/rmstore/FSRMStateRoot/RMDTSecretManagerRoot. Name node is in safe mode.
The reported blocks 5 has reached the threshold 0.9990 of total blocks 5. The number of live datanodes 1 has reached the minimum number 0. In safe mode extension. Safe mode will be turned off automatically in 18 seconds.
2020-10-21 12:46:30,927 INFO org.apache.hadoop.ipc.Server: IPC Server handler 7 on 9000, call org.apache.hadoop.hdfs.protocol.ClientProtocol.mkdirs from 172.21.0.9:53982 Call#0 Retry#2: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot create directory /tmp/hadoop-root/yarn/system/rmstore/FSRMStateRoot/RMDTSecretManagerRoot. Name node is in safe mode.
The reported blocks 5 has reached the threshold 0.9990 of total blocks 5. The number of live datanodes 1 has reached the minimum number 0. In safe mode extension. Safe mode will be turned off automatically in 15 seconds.
2020-10-21 12:46:32,320 INFO org.apache.hadoop.ipc.Server: IPC Server handler 9 on 9000, call org.apache.hadoop.hdfs.protocol.ClientProtocol.mkdirs from 172.21.0.9:53982 Call#0 Retry#3: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot create directory /tmp/hadoop-root/yarn/system/rmstore/FSRMStateRoot/RMDTSecretManagerRoot. Name node is in safe mode.
The reported blocks 5 has reached the threshold 0.9990 of total blocks 5. The number of live datanodes 1 has reached the minimum number 0. In safe mode extension. Safe mode will be turned off automatically in 14 seconds.
2020-10-21 12:46:33,934 INFO org.apache.hadoop.ipc.Server: IPC Server handler 7 on 9000, call org.apache.hadoop.hdfs.protocol.ClientProtocol.mkdirs from 172.21.0.9:53982 Call#0 Retry#4: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot create directory /tmp/hadoop-root/yarn/system/rmstore/FSRMStateRoot/RMDTSecretManagerRoot. Name node is in safe mode.
The reported blocks 5 has reached the threshold 0.9990 of total blocks 5. The number of live datanodes 1 has reached the minimum number 0. In safe mode extension. Safe mode will be turned off automatically in 12 seconds.
2020-10-21 12:46:35,248 INFO org.apache.hadoop.ipc.Server: IPC Server handler 9 on 9000, call org.apache.hadoop.hdfs.protocol.ClientProtocol.mkdirs from 172.21.0.9:53982 Call#0 Retry#5: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot create directory /tmp/hadoop-root/yarn/system/rmstore/FSRMStateRoot/RMDTSecretManagerRoot. Name node is in safe mode.
The reported blocks 5 has reached the threshold 0.9990 of total blocks 5. The number of live datanodes 1 has reached the minimum number 0. In safe mode extension. Safe mode will be turned off automatically in 11 seconds.
2020-10-21 12:46:36,610 INFO org.apache.hadoop.hdfs.StateChange: STATE* Safe mode ON, in safe mode extension. 
The reported blocks 5 has reached the threshold 0.9990 of total blocks 5. The number of live datanodes 1 has reached the minimum number 0. In safe mode extension. Safe mode will be turned off automatically in 9 seconds.
2020-10-21 12:46:36,713 INFO org.apache.hadoop.ipc.Server: IPC Server handler 7 on 9000, call org.apache.hadoop.hdfs.protocol.ClientProtocol.mkdirs from 172.21.0.9:53982 Call#0 Retry#6: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot create directory /tmp/hadoop-root/yarn/system/rmstore/FSRMStateRoot/RMDTSecretManagerRoot. Name node is in safe mode.
The reported blocks 5 has reached the threshold 0.9990 of total blocks 5. The number of live datanodes 1 has reached the minimum number 0. In safe mode extension. Safe mode will be turned off automatically in 9 seconds.
2020-10-21 12:46:38,918 INFO org.apache.hadoop.ipc.Server: IPC Server handler 9 on 9000, call org.apache.hadoop.hdfs.protocol.ClientProtocol.mkdirs from 172.21.0.9:53982 Call#0 Retry#7: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot create directory /tmp/hadoop-root/yarn/system/rmstore/FSRMStateRoot/RMDTSecretManagerRoot. Name node is in safe mode.
The reported blocks 5 has reached the threshold 0.9990 of total blocks 5. The number of live datanodes 1 has reached the minimum number 0. In safe mode extension. Safe mode will be turned off automatically in 7 seconds.
2020-10-21 12:46:40,209 INFO org.apache.hadoop.ipc.Server: IPC Server handler 8 on 9000, call org.apache.hadoop.hdfs.protocol.ClientProtocol.mkdirs from 172.21.0.9:53982 Call#0 Retry#8: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot create directory /tmp/hadoop-root/yarn/system/rmstore/FSRMStateRoot/RMDTSecretManagerRoot. Name node is in safe mode.
The reported blocks 5 has reached the threshold 0.9990 of total blocks 5. The number of live datanodes 1 has reached the minimum number 0. In safe mode extension. Safe mode will be turned off automatically in 6 seconds.
2020-10-21 12:46:42,321 INFO org.apache.hadoop.ipc.Server: IPC Server handler 8 on 9000, call org.apache.hadoop.hdfs.protocol.ClientProtocol.mkdirs from 172.21.0.9:53982 Call#0 Retry#9: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot create directory /tmp/hadoop-root/yarn/system/rmstore/FSRMStateRoot/RMDTSecretManagerRoot. Name node is in safe mode.
The reported blocks 5 has reached the threshold 0.9990 of total blocks 5. The number of live datanodes 1 has reached the minimum number 0. In safe mode extension. Safe mode will be turned off automatically in 4 seconds.
2020-10-21 12:46:45,915 INFO org.apache.hadoop.ipc.Server: IPC Server handler 0 on 9000, call org.apache.hadoop.hdfs.protocol.ClientProtocol.mkdirs from 172.21.0.9:53982 Call#0 Retry#10: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot create directory /tmp/hadoop-root/yarn/system/rmstore/FSRMStateRoot/RMDTSecretManagerRoot. Name node is in safe mode.
The reported blocks 5 has reached the threshold 0.9990 of total blocks 5. The number of live datanodes 1 has reached the minimum number 0. In safe mode extension. Safe mode will be turned off automatically in 0 seconds.
2020-10-21 12:46:46,904 INFO org.apache.hadoop.hdfs.StateChange: STATE* Leaving safe mode after 34 secs
2020-10-21 12:46:47,014 INFO org.apache.hadoop.hdfs.StateChange: STATE* Safe mode is OFF
2020-10-21 12:46:47,064 INFO org.apache.hadoop.hdfs.StateChange: STATE* Network topology has 1 racks and 1 datanodes
2020-10-21 12:46:47,093 INFO org.apache.hadoop.hdfs.StateChange: STATE* UnderReplicatedBlocks has 0 blocks
2020-10-21 12:46:53,259 INFO org.apache.hadoop.hdfs.server.namenode.EditLogFileOutputStream: Nothing to flush
2020-10-21 12:47:01,623 INFO org.apache.hadoop.hdfs.server.namenode.EditLogFileOutputStream: Nothing to flush
2020-10-21 12:47:02,190 INFO org.apache.hadoop.hdfs.server.namenode.EditLogFileOutputStream: Nothing to flush
2020-10-21 12:47:26,625 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 2 Total time for transactions(ms): 13 Number of transactions batched in Syncs: 0 Number of syncs: 5 SyncTimes(ms): 41 
2020-10-21 12:47:26,667 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741830_1006{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-f81e3095-598a-4077-8723-10720198009d:NORMAL:172.21.0.9:50010|RBW]]} for /tmp/hadoop-root/yarn/system/rmstore/FSRMStateRoot/EpochNode.new.tmp
2020-10-21 12:47:27,080 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741830_1006{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-f81e3095-598a-4077-8723-10720198009d:NORMAL:172.21.0.9:50010|RBW]]} is not COMPLETE (ucState = COMMITTED, replication# = 0 <  minimum = 1) in file /tmp/hadoop-root/yarn/system/rmstore/FSRMStateRoot/EpochNode.new.tmp
2020-10-21 12:47:27,080 INFO org.apache.hadoop.hdfs.server.namenode.EditLogFileOutputStream: Nothing to flush
2020-10-21 12:47:27,140 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.21.0.9:50010 is added to blk_1073741830_1006{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-f81e3095-598a-4077-8723-10720198009d:NORMAL:172.21.0.9:50010|RBW]]} size 2
2020-10-21 12:47:27,502 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-root/yarn/system/rmstore/FSRMStateRoot/EpochNode.new.tmp is closed by DFSClient_NONMAPREDUCE_1325193736_1
2020-10-21 12:47:27,549 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741826_1002 172.21.0.9:50010 
2020-10-21 12:47:27,707 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741831_1007{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-f81e3095-598a-4077-8723-10720198009d:NORMAL:172.21.0.9:50010|RBW]]} for /tmp/hadoop-root/yarn/system/rmstore/FSRMStateRoot/RMDTSecretManagerRoot/DelegationKey_3.tmp
2020-10-21 12:47:27,743 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.21.0.9:50010 is added to blk_1073741831_1007{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-f81e3095-598a-4077-8723-10720198009d:NORMAL:172.21.0.9:50010|RBW]]} size 0
2020-10-21 12:47:27,745 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-root/yarn/system/rmstore/FSRMStateRoot/RMDTSecretManagerRoot/DelegationKey_3.tmp is closed by DFSClient_NONMAPREDUCE_1325193736_1
2020-10-21 12:47:27,768 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741832_1008{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-f81e3095-598a-4077-8723-10720198009d:NORMAL:172.21.0.9:50010|RBW]]} for /tmp/hadoop-root/yarn/system/rmstore/FSRMStateRoot/RMDTSecretManagerRoot/DelegationKey_4.tmp
2020-10-21 12:47:27,825 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.21.0.9:50010 is added to blk_1073741832_1008{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-f81e3095-598a-4077-8723-10720198009d:NORMAL:172.21.0.9:50010|RBW]]} size 0
2020-10-21 12:47:27,828 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-root/yarn/system/rmstore/FSRMStateRoot/RMDTSecretManagerRoot/DelegationKey_4.tmp is closed by DFSClient_NONMAPREDUCE_1325193736_1
2020-10-21 12:47:28,874 INFO BlockStateChange: BLOCK* BlockManager: ask 172.21.0.9:50010 to delete [blk_1073741826_1002]
2020-10-21 12:47:50,523 INFO org.apache.hadoop.util.JvmPauseMonitor: Detected pause in JVM or host machine (eg GC): pause of approximately 1104ms
No GCs detected
2020-10-21 12:48:22,632 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741833_1009{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-f81e3095-598a-4077-8723-10720198009d:NORMAL:172.21.0.9:50010|RBW]]} for /data/flume/flume.log-2020-10-21-12-48-19.1603284499242.tmp
2020-10-21 12:48:23,052 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* fsync: /data/flume/flume.log-2020-10-21-12-48-19.1603284499242.tmp for DFSClient_NONMAPREDUCE_-1134862284_28
2020-10-21 12:48:23,062 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741834_1010{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-f81e3095-598a-4077-8723-10720198009d:NORMAL:172.21.0.9:50010|RBW]]} for /data/flume/flume.log-2020-10-21-12-48-22.1603284502255.tmp
2020-10-21 12:48:23,112 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* fsync: /data/flume/flume.log-2020-10-21-12-48-22.1603284502255.tmp for DFSClient_NONMAPREDUCE_-1134862284_28
2020-10-21 12:48:25,866 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741835_1011{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-f81e3095-598a-4077-8723-10720198009d:NORMAL:172.21.0.9:50010|RBW]]} for /data/profiles.json._COPYING_
2020-10-21 12:48:26,051 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741835_1011{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-f81e3095-598a-4077-8723-10720198009d:NORMAL:172.21.0.9:50010|RBW]]} is not COMPLETE (ucState = COMMITTED, replication# = 0 <  minimum = 1) in file /data/profiles.json._COPYING_
2020-10-21 12:48:26,052 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.21.0.9:50010 is added to blk_1073741835_1011{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-f81e3095-598a-4077-8723-10720198009d:NORMAL:172.21.0.9:50010|RBW]]} size 161820
2020-10-21 12:48:26,226 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741836_1012{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-f81e3095-598a-4077-8723-10720198009d:NORMAL:172.21.0.9:50010|RBW]]} for /data/flume/flume.log-2020-10-21-12-48-23.1603284503116.tmp
2020-10-21 12:48:26,247 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* fsync: /data/flume/flume.log-2020-10-21-12-48-23.1603284503116.tmp for DFSClient_NONMAPREDUCE_-1134862284_28
2020-10-21 12:48:26,461 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /data/profiles.json._COPYING_ is closed by DFSClient_NONMAPREDUCE_-1077480024_1
2020-10-21 12:48:29,991 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 51 Total time for transactions(ms): 26 Number of transactions batched in Syncs: 1 Number of syncs: 39 SyncTimes(ms): 79 
2020-10-21 12:48:52,687 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.21.0.9:50010 is added to blk_1073741833_1009{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-f81e3095-598a-4077-8723-10720198009d:NORMAL:172.21.0.9:50010|RBW]]} size 55
2020-10-21 12:48:52,755 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /data/flume/flume.log-2020-10-21-12-48-19.1603284499242.tmp is closed by DFSClient_NONMAPREDUCE_-1134862284_28
2020-10-21 12:48:53,803 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.21.0.9:50010 is added to blk_1073741834_1010{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-f81e3095-598a-4077-8723-10720198009d:NORMAL:172.21.0.9:50010|RBW]]} size 5573
2020-10-21 12:48:54,303 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /data/flume/flume.log-2020-10-21-12-48-22.1603284502255.tmp is closed by DFSClient_NONMAPREDUCE_-1134862284_28
2020-10-21 12:48:54,467 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.21.0.9:50010 is added to blk_1073741836_1012{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-f81e3095-598a-4077-8723-10720198009d:NORMAL:172.21.0.9:50010|RBW]]} size 622
2020-10-21 12:48:54,589 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /data/flume/flume.log-2020-10-21-12-48-23.1603284503116.tmp is closed by DFSClient_NONMAPREDUCE_-1134862284_28
2020-10-21 12:49:02,417 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741837_1013{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-f81e3095-598a-4077-8723-10720198009d:NORMAL:172.21.0.9:50010|RBW]]} for /data/flume/flume.log-2020-10-21-12-48-56.1603284536299.tmp
2020-10-21 12:49:02,438 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* fsync: /data/flume/flume.log-2020-10-21-12-48-56.1603284536299.tmp for DFSClient_NONMAPREDUCE_-1134862284_28
2020-10-21 12:49:02,443 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741838_1014{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-f81e3095-598a-4077-8723-10720198009d:NORMAL:172.21.0.9:50010|RBW]]} for /data/flume/flume.log-2020-10-21-12-48-57.1603284537626.tmp
2020-10-21 12:49:02,462 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* fsync: /data/flume/flume.log-2020-10-21-12-48-57.1603284537626.tmp for DFSClient_NONMAPREDUCE_-1134862284_28
2020-10-21 12:49:02,467 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741839_1015{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-f81e3095-598a-4077-8723-10720198009d:NORMAL:172.21.0.9:50010|RBW]]} for /data/flume/flume.log-2020-10-21-12-49-02.1603284542339.tmp
2020-10-21 12:49:02,484 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* fsync: /data/flume/flume.log-2020-10-21-12-49-02.1603284542339.tmp for DFSClient_NONMAPREDUCE_-1134862284_28
2020-10-21 12:49:21,710 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741840_1016{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-f81e3095-598a-4077-8723-10720198009d:NORMAL:172.21.0.9:50010|RBW]]} for /data/flume/flume.log-2020-10-21-12-49-21.1603284561634.tmp
2020-10-21 12:49:21,728 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* fsync: /data/flume/flume.log-2020-10-21-12-49-21.1603284561634.tmp for DFSClient_NONMAPREDUCE_-1134862284_28
2020-10-21 12:49:27,643 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.21.0.9:50010 is added to blk_1073741837_1013{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-f81e3095-598a-4077-8723-10720198009d:NORMAL:172.21.0.9:50010|RBW]]} size 56
2020-10-21 12:49:27,645 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /data/flume/flume.log-2020-10-21-12-48-56.1603284536299.tmp is closed by DFSClient_NONMAPREDUCE_-1134862284_28
2020-10-21 12:49:32,357 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.21.0.9:50010 is added to blk_1073741838_1014{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-f81e3095-598a-4077-8723-10720198009d:NORMAL:172.21.0.9:50010|RBW]]} size 54
2020-10-21 12:49:32,359 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 88 Total time for transactions(ms): 156 Number of transactions batched in Syncs: 1 Number of syncs: 68 SyncTimes(ms): 124 
2020-10-21 12:49:32,360 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /data/flume/flume.log-2020-10-21-12-48-57.1603284537626.tmp is closed by DFSClient_NONMAPREDUCE_-1134862284_28
2020-10-21 12:49:32,410 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.21.0.9:50010 is added to blk_1073741839_1015{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-f81e3095-598a-4077-8723-10720198009d:NORMAL:172.21.0.9:50010|RBW]]} size 5547
2020-10-21 12:49:32,413 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /data/flume/flume.log-2020-10-21-12-49-02.1603284542339.tmp is closed by DFSClient_NONMAPREDUCE_-1134862284_28
2020-10-21 12:49:51,701 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.21.0.9:50010 is added to blk_1073741840_1016{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-f81e3095-598a-4077-8723-10720198009d:NORMAL:172.21.0.9:50010|RBW]]} size 5655
2020-10-21 12:49:51,707 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /data/flume/flume.log-2020-10-21-12-49-21.1603284561634.tmp is closed by DFSClient_NONMAPREDUCE_-1134862284_28
2020-10-21 12:49:54,825 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741841_1017{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-f81e3095-598a-4077-8723-10720198009d:NORMAL:172.21.0.9:50010|RBW]]} for /data/flume/flume.log-2020-10-21-12-49-54.1603284594753.tmp
2020-10-21 12:49:54,840 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* fsync: /data/flume/flume.log-2020-10-21-12-49-54.1603284594753.tmp for DFSClient_NONMAPREDUCE_-1134862284_28
2020-10-21 12:50:21,796 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741842_1018{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-f81e3095-598a-4077-8723-10720198009d:NORMAL:172.21.0.9:50010|RBW]]} for /data/flume/flume.log-2020-10-21-12-50-21.1603284621710.tmp
2020-10-21 12:50:21,810 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* fsync: /data/flume/flume.log-2020-10-21-12-50-21.1603284621710.tmp for DFSClient_NONMAPREDUCE_-1134862284_28
2020-10-21 12:50:24,823 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.21.0.9:50010 is added to blk_1073741841_1017{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-f81e3095-598a-4077-8723-10720198009d:NORMAL:172.21.0.9:50010|RBW]]} size 5671
2020-10-21 12:50:24,825 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /data/flume/flume.log-2020-10-21-12-49-54.1603284594753.tmp is closed by DFSClient_NONMAPREDUCE_-1134862284_28
2020-10-21 12:52:38,686 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: STARTUP_MSG: 
/************************************************************
STARTUP_MSG: Starting NameNode
STARTUP_MSG:   host = master/172.22.0.7
STARTUP_MSG:   args = []
STARTUP_MSG:   version = 2.7.3
STARTUP_MSG:   classpath = /usr/local/hadoop/etc/hadoop/:/usr/local/hadoop/share/hadoop/common/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/common/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/common/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-digester-1.8.jar:/usr/local/hadoop/share/hadoop/common/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/gson-2.2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/api-asn1-api-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-core-1.8.0.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-framework-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-configuration-1.6.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-kerberos-codec-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-httpclient-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/api-util-1.0.0-M20.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-recipes-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/common/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/common/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/common/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-collections-3.2.2.jar:/usr/local/hadoop/share/hadoop/common/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-annotations-2.7.3.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-api-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/common/lib/hadoop-auth-2.7.3.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/common/lib/curator-client-2.7.1.jar:/usr/local/hadoop/share/hadoop/common/lib/slf4j-log4j12-1.7.10.jar:/usr/local/hadoop/share/hadoop/common/lib/httpcore-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/httpclient-4.2.5.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/java-xmlbuilder-0.4.jar:/usr/local/hadoop/share/hadoop/common/lib/jets3t-0.9.0.jar:/usr/local/hadoop/share/hadoop/common/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/common/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-net-3.1.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-beanutils-1.7.0.jar:/usr/local/hadoop/share/hadoop/common/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/common/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/common/lib/jsch-0.1.42.jar:/usr/local/hadoop/share/hadoop/common/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/common/lib/apacheds-i18n-2.0.0-M15.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.3-tests.jar:/usr/local/hadoop/share/hadoop/common/hadoop-common-2.7.3.jar:/usr/local/hadoop/share/hadoop/common/hadoop-nfs-2.7.3.jar:/usr/local/hadoop/share/hadoop/hdfs:/usr/local/hadoop/share/hadoop/hdfs/lib/xercesImpl-2.9.1.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/htrace-core-3.1.0-incubating.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xml-apis-1.3.04.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/xmlenc-0.52.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-nfs-2.7.3.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.3.jar:/usr/local/hadoop/share/hadoop/hdfs/hadoop-hdfs-2.7.3-tests.jar:/usr/local/hadoop/share/hadoop/yarn/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-logging-1.1.3.jar:/usr/local/hadoop/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/zookeeper-3.4.6-tests.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jsr305-3.0.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jettison-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-client-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-json-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/activation-1.1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/servlet-api-2.5.jar:/usr/local/hadoop/share/hadoop/yarn/lib/guava-11.0.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/stax-api-1.0-2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-collections-3.2.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-impl-2.2.3-1.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-cli-1.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-jaxrs-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-lang-2.6.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jetty-util-6.1.26.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/yarn/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jackson-xc-1.9.13.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-codec-1.4.jar:/usr/local/hadoop/share/hadoop/yarn/lib/jaxb-api-2.2.2.jar:/usr/local/hadoop/share/hadoop/yarn/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-2.7.3.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-nodemanager-2.7.3.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-common-2.7.3.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-2.7.3.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-2.7.3.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-client-2.7.3.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-registry-2.7.3.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-tests-2.7.3.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-2.7.3.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-2.7.3.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-common-2.7.3.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-api-2.7.3.jar:/usr/local/hadoop/share/hadoop/yarn/hadoop-yarn-server-web-proxy-2.7.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/asm-3.2.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/junit-4.11.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-mapper-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/aopalliance-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/snappy-java-1.0.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hamcrest-core-1.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/paranamer-2.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-compress-1.4.1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/log4j-1.2.17.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/javax.inject-1.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/guice-servlet-3.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-server-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/protobuf-java-2.5.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/netty-3.6.2.Final.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-core-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jackson-core-asl-1.9.13.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/hadoop-annotations-2.7.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/avro-1.7.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/leveldbjni-all-1.8.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/jersey-guice-1.9.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/xz-1.0.jar:/usr/local/hadoop/share/hadoop/mapreduce/lib/commons-io-2.4.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-2.7.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-2.7.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-common-2.7.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-2.7.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-2.7.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-core-2.7.3.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-2.7.3-tests.jar:/usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-client-app-2.7.3.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar:/usr/local/hadoop/contrib/capacity-scheduler/*.jar
STARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r baa91f7c6bc9cb92be5982de4719c1c8af91ccff; compiled by 'root' on 2016-08-18T01:41Z
STARTUP_MSG:   java = 1.8.0_151
************************************************************/
2020-10-21 12:52:38,705 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]
2020-10-21 12:52:38,714 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: createNameNode []
2020-10-21 12:52:39,285 INFO org.apache.hadoop.metrics2.impl.MetricsConfig: loaded properties from hadoop-metrics2.properties
2020-10-21 12:52:39,376 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2020-10-21 12:52:39,377 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: NameNode metrics system started
2020-10-21 12:52:39,379 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: fs.defaultFS is hdfs://master:9000
2020-10-21 12:52:39,380 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Clients are to use master:9000 to access this namenode/service.
2020-10-21 12:52:39,547 INFO org.apache.hadoop.hdfs.DFSUtil: Starting Web-server for hdfs at: http://master:50070
2020-10-21 12:52:39,600 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2020-10-21 12:52:39,610 INFO org.apache.hadoop.security.authentication.server.AuthenticationFilter: Unable to initialize FileSignerSecretProvider, falling back to use random secrets.
2020-10-21 12:52:39,616 INFO org.apache.hadoop.http.HttpRequestLog: Http request log for http.requests.namenode is not defined
2020-10-21 12:52:39,623 INFO org.apache.hadoop.http.HttpServer2: Added global filter 'safety' (class=org.apache.hadoop.http.HttpServer2$QuotingInputFilter)
2020-10-21 12:52:39,627 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context hdfs
2020-10-21 12:52:39,627 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context static
2020-10-21 12:52:39,628 INFO org.apache.hadoop.http.HttpServer2: Added filter static_user_filter (class=org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter) to context logs
2020-10-21 12:52:39,813 INFO org.apache.hadoop.http.HttpServer2: Added filter 'org.apache.hadoop.hdfs.web.AuthFilter' (class=org.apache.hadoop.hdfs.web.AuthFilter)
2020-10-21 12:52:39,816 INFO org.apache.hadoop.http.HttpServer2: addJerseyResourcePackage: packageName=org.apache.hadoop.hdfs.server.namenode.web.resources;org.apache.hadoop.hdfs.web.resources, pathSpec=/webhdfs/v1/*
2020-10-21 12:52:39,840 INFO org.apache.hadoop.http.HttpServer2: Jetty bound to port 50070
2020-10-21 12:52:39,840 INFO org.mortbay.log: jetty-6.1.26
2020-10-21 12:52:40,016 INFO org.mortbay.log: Started HttpServer2$SelectChannelConnectorWithSafeStartup@master:50070
2020-10-21 12:52:40,043 WARN org.apache.hadoop.hdfs.server.common.Util: Path /data/dfs/name should be specified as a URI in configuration files. Please update hdfs configuration.
2020-10-21 12:52:40,045 WARN org.apache.hadoop.hdfs.server.common.Util: Path /data/dfs/name should be specified as a URI in configuration files. Please update hdfs configuration.
2020-10-21 12:52:40,046 WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Only one image storage directory (dfs.namenode.name.dir) configured. Beware of data loss due to lack of redundant storage directories!
2020-10-21 12:52:40,046 WARN org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Only one namespace edits storage directory (dfs.namenode.edits.dir) configured. Beware of data loss due to lack of redundant storage directories!
2020-10-21 12:52:40,051 WARN org.apache.hadoop.hdfs.server.common.Util: Path /data/dfs/name should be specified as a URI in configuration files. Please update hdfs configuration.
2020-10-21 12:52:40,053 WARN org.apache.hadoop.hdfs.server.common.Util: Path /data/dfs/name should be specified as a URI in configuration files. Please update hdfs configuration.
2020-10-21 12:52:40,083 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: No KeyProvider found.
2020-10-21 12:52:40,083 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsLock is fair:true
2020-10-21 12:52:40,127 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.block.invalidate.limit=1000
2020-10-21 12:52:40,127 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true
2020-10-21 12:52:40,129 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000
2020-10-21 12:52:40,129 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: The block deletion will start around 2020 Oct 21 12:52:40
2020-10-21 12:52:40,131 INFO org.apache.hadoop.util.GSet: Computing capacity for map BlocksMap
2020-10-21 12:52:40,132 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2020-10-21 12:52:40,133 INFO org.apache.hadoop.util.GSet: 2.0% max memory 889 MB = 17.8 MB
2020-10-21 12:52:40,134 INFO org.apache.hadoop.util.GSet: capacity      = 2^21 = 2097152 entries
2020-10-21 12:52:40,139 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: dfs.block.access.token.enable=false
2020-10-21 12:52:40,140 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: defaultReplication         = 1
2020-10-21 12:52:40,140 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplication             = 512
2020-10-21 12:52:40,140 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: minReplication             = 1
2020-10-21 12:52:40,141 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxReplicationStreams      = 2
2020-10-21 12:52:40,141 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: replicationRecheckInterval = 3000
2020-10-21 12:52:40,141 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: encryptDataTransfer        = false
2020-10-21 12:52:40,142 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: maxNumBlocksToLog          = 1000
2020-10-21 12:52:40,149 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: fsOwner             = root (auth:SIMPLE)
2020-10-21 12:52:40,150 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: supergroup          = supergroup
2020-10-21 12:52:40,150 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: isPermissionEnabled = false
2020-10-21 12:52:40,151 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: HA Enabled: false
2020-10-21 12:52:40,153 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Append Enabled: true
2020-10-21 12:52:40,229 INFO org.apache.hadoop.util.GSet: Computing capacity for map INodeMap
2020-10-21 12:52:40,229 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2020-10-21 12:52:40,230 INFO org.apache.hadoop.util.GSet: 1.0% max memory 889 MB = 8.9 MB
2020-10-21 12:52:40,230 INFO org.apache.hadoop.util.GSet: capacity      = 2^20 = 1048576 entries
2020-10-21 12:52:40,231 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: ACLs enabled? false
2020-10-21 12:52:40,232 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: XAttrs enabled? true
2020-10-21 12:52:40,232 INFO org.apache.hadoop.hdfs.server.namenode.FSDirectory: Maximum size of an xattr: 16384
2020-10-21 12:52:40,233 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: Caching file names occuring more than 10 times
2020-10-21 12:52:40,242 INFO org.apache.hadoop.util.GSet: Computing capacity for map cachedBlocks
2020-10-21 12:52:40,243 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2020-10-21 12:52:40,243 INFO org.apache.hadoop.util.GSet: 0.25% max memory 889 MB = 2.2 MB
2020-10-21 12:52:40,244 INFO org.apache.hadoop.util.GSet: capacity      = 2^18 = 262144 entries
2020-10-21 12:52:40,246 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.threshold-pct = 0.9990000128746033
2020-10-21 12:52:40,246 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.min.datanodes = 0
2020-10-21 12:52:40,247 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: dfs.namenode.safemode.extension     = 30000
2020-10-21 12:52:40,253 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10
2020-10-21 12:52:40,254 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10
2020-10-21 12:52:40,254 INFO org.apache.hadoop.hdfs.server.namenode.top.metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25
2020-10-21 12:52:40,257 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Retry cache on namenode is enabled
2020-10-21 12:52:40,257 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis
2020-10-21 12:52:40,260 INFO org.apache.hadoop.util.GSet: Computing capacity for map NameNodeRetryCache
2020-10-21 12:52:40,260 INFO org.apache.hadoop.util.GSet: VM type       = 64-bit
2020-10-21 12:52:40,261 INFO org.apache.hadoop.util.GSet: 0.029999999329447746% max memory 889 MB = 273.1 KB
2020-10-21 12:52:40,261 INFO org.apache.hadoop.util.GSet: capacity      = 2^15 = 32768 entries
2020-10-21 12:52:40,279 INFO org.apache.hadoop.hdfs.server.common.Storage: Lock on /data/dfs/name/in_use.lock acquired by nodename 235@master
2020-10-21 12:52:40,362 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Recovering unfinalized segments in /data/dfs/name/current
2020-10-21 12:52:40,532 INFO org.apache.hadoop.hdfs.server.namenode.FileJournalManager: Finalizing edits file /data/dfs/name/current/edits_inprogress_0000000000000000041 -> /data/dfs/name/current/edits_0000000000000000041-0000000000000000145
2020-10-21 12:52:40,544 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Planning to load image: FSImageFile(file=/data/dfs/name/current/fsimage_0000000000000000000, cpktTxId=0000000000000000000)
2020-10-21 12:52:40,692 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatPBINode: Loading 1 INodes.
2020-10-21 12:52:40,750 INFO org.apache.hadoop.hdfs.server.namenode.FSImageFormatProtobuf: Loaded FSImage in 0 seconds.
2020-10-21 12:52:40,752 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Loaded image for txid 0 from /data/dfs/name/current/fsimage_0000000000000000000
2020-10-21 12:52:40,754 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@312ab28e expecting start txid #1
2020-10-21 12:52:40,755 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file /data/dfs/name/current/edits_0000000000000000001-0000000000000000040
2020-10-21 12:52:40,759 INFO org.apache.hadoop.hdfs.server.namenode.EditLogInputStream: Fast-forwarding stream '/data/dfs/name/current/edits_0000000000000000001-0000000000000000040' to transaction ID 1
2020-10-21 12:52:40,819 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file /data/dfs/name/current/edits_0000000000000000001-0000000000000000040 of size 1048576 edits # 40 loaded in 0 seconds
2020-10-21 12:52:40,820 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Reading org.apache.hadoop.hdfs.server.namenode.RedundantEditLogInputStream@5644dc81 expecting start txid #41
2020-10-21 12:52:40,821 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Start loading edits file /data/dfs/name/current/edits_0000000000000000041-0000000000000000145
2020-10-21 12:52:40,822 INFO org.apache.hadoop.hdfs.server.namenode.EditLogInputStream: Fast-forwarding stream '/data/dfs/name/current/edits_0000000000000000041-0000000000000000145' to transaction ID 1
2020-10-21 12:52:40,851 INFO org.apache.hadoop.hdfs.server.namenode.FSImage: Edits file /data/dfs/name/current/edits_0000000000000000041-0000000000000000145 of size 1048576 edits # 105 loaded in 0 seconds
2020-10-21 12:52:40,853 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Need to save fs image? false (staleImage=false, haEnabled=false, isRollingUpgrade=false)
2020-10-21 12:52:40,856 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Starting log segment at 146
2020-10-21 12:52:40,947 INFO org.apache.hadoop.hdfs.server.namenode.NameCache: initialized with 0 entries 0 lookups
2020-10-21 12:52:40,948 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Finished loading FSImage in 681 msecs
2020-10-21 12:52:41,142 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: RPC server is binding to master:9000
2020-10-21 12:52:41,150 INFO org.apache.hadoop.ipc.CallQueueManager: Using callQueue class java.util.concurrent.LinkedBlockingQueue
2020-10-21 12:52:41,164 INFO org.apache.hadoop.ipc.Server: Starting Socket Reader #1 for port 9000
2020-10-21 12:52:41,193 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Registered FSNamesystemState MBean
2020-10-21 12:52:41,195 WARN org.apache.hadoop.hdfs.server.common.Util: Path /data/dfs/name should be specified as a URI in configuration files. Please update hdfs configuration.
2020-10-21 12:52:41,212 INFO org.apache.hadoop.hdfs.server.namenode.LeaseManager: Number of blocks under construction: 1
2020-10-21 12:52:41,213 INFO org.apache.hadoop.hdfs.server.namenode.LeaseManager: Number of blocks under construction: 1
2020-10-21 12:52:41,214 INFO org.apache.hadoop.hdfs.StateChange: STATE* Safe mode ON. 
The reported blocks 0 needs additional 16 blocks to reach the threshold 0.9990 of total blocks 16.
The number of live datanodes 0 has reached the minimum number 0. Safe mode will be turned off automatically once the thresholds have been reached.
2020-10-21 12:52:41,222 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2020-10-21 12:52:41,262 INFO org.apache.hadoop.ipc.Server: IPC Server Responder: starting
2020-10-21 12:52:41,264 INFO org.apache.hadoop.ipc.Server: IPC Server listener on 9000: starting
2020-10-21 12:52:41,267 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: NameNode RPC up at: master/172.22.0.7:9000
2020-10-21 12:52:41,267 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: Starting services required for active state
2020-10-21 12:52:41,274 INFO org.apache.hadoop.hdfs.server.blockmanagement.CacheReplicationMonitor: Starting CacheReplicationMonitor with interval 30000 milliseconds
2020-10-21 12:52:44,922 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* registerDatanode: from DatanodeRegistration(172.22.0.7:50010, datanodeUuid=6158f795-6f5a-4077-b880-90e3ab329bf6, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-3e8c1414-db15-4700-b0c2-df0389673a7b;nsid=2041325608;c=0) storage 6158f795-6f5a-4077-b880-90e3ab329bf6
2020-10-21 12:52:44,923 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2020-10-21 12:52:44,924 INFO org.apache.hadoop.net.NetworkTopology: Adding a new node: /default-rack/172.22.0.7:50010
2020-10-21 12:52:44,991 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Number of failed storage changes from 0 to 0
2020-10-21 12:52:44,992 INFO org.apache.hadoop.hdfs.server.blockmanagement.DatanodeDescriptor: Adding new storage ID DS-f81e3095-598a-4077-8723-10720198009d for DN 172.22.0.7:50010
2020-10-21 12:52:45,024 INFO org.apache.hadoop.hdfs.StateChange: STATE* Safe mode extension entered. 
The reported blocks 15 has reached the threshold 0.9990 of total blocks 16. The number of live datanodes 1 has reached the minimum number 0. In safe mode extension. Safe mode will be turned off automatically in 29 seconds.
2020-10-21 12:52:45,025 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: initializing replication queues
2020-10-21 12:52:45,027 INFO BlockStateChange: BLOCK* processReport: from storage DS-f81e3095-598a-4077-8723-10720198009d node DatanodeRegistration(172.22.0.7:50010, datanodeUuid=6158f795-6f5a-4077-b880-90e3ab329bf6, infoPort=50075, infoSecurePort=0, ipcPort=50020, storageInfo=lv=-56;cid=CID-3e8c1414-db15-4700-b0c2-df0389673a7b;nsid=2041325608;c=0), blocks: 17, hasStaleStorage: false, processing time: 6 msecs
2020-10-21 12:52:45,038 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Total number of blocks            = 17
2020-10-21 12:52:45,039 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of invalid blocks          = 0
2020-10-21 12:52:45,040 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of under-replicated blocks = 0
2020-10-21 12:52:45,040 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of  over-replicated blocks = 0
2020-10-21 12:52:45,040 INFO org.apache.hadoop.hdfs.server.blockmanagement.BlockManager: Number of blocks being written    = 1
2020-10-21 12:52:45,041 INFO org.apache.hadoop.hdfs.StateChange: STATE* Replication Queue initialization scan for invalid, over- and under-replicated blocks completed in 15 msec
2020-10-21 12:52:54,330 INFO org.apache.hadoop.ipc.Server: IPC Server handler 0 on 9000, call org.apache.hadoop.hdfs.protocol.ClientProtocol.mkdirs from 172.22.0.7:56742 Call#0 Retry#0: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot create directory /tmp/hadoop-root/yarn/system/rmstore/FSRMStateRoot/RMDTSecretManagerRoot. Name node is in safe mode.
The reported blocks 16 has reached the threshold 0.9990 of total blocks 16. The number of live datanodes 1 has reached the minimum number 0. In safe mode extension. Safe mode will be turned off automatically in 20 seconds.
2020-10-21 12:52:57,338 INFO org.apache.hadoop.ipc.Server: IPC Server handler 2 on 9000, call org.apache.hadoop.hdfs.protocol.ClientProtocol.mkdirs from 172.22.0.7:56742 Call#0 Retry#1: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot create directory /tmp/hadoop-root/yarn/system/rmstore/FSRMStateRoot/RMDTSecretManagerRoot. Name node is in safe mode.
The reported blocks 16 has reached the threshold 0.9990 of total blocks 16. The number of live datanodes 1 has reached the minimum number 0. In safe mode extension. Safe mode will be turned off automatically in 17 seconds.
2020-10-21 12:52:59,056 INFO org.apache.hadoop.ipc.Server: IPC Server handler 3 on 9000, call org.apache.hadoop.hdfs.protocol.ClientProtocol.mkdirs from 172.22.0.7:56742 Call#0 Retry#2: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot create directory /tmp/hadoop-root/yarn/system/rmstore/FSRMStateRoot/RMDTSecretManagerRoot. Name node is in safe mode.
The reported blocks 16 has reached the threshold 0.9990 of total blocks 16. The number of live datanodes 1 has reached the minimum number 0. In safe mode extension. Safe mode will be turned off automatically in 15 seconds.
2020-10-21 12:53:00,652 INFO org.apache.hadoop.ipc.Server: IPC Server handler 1 on 9000, call org.apache.hadoop.hdfs.protocol.ClientProtocol.mkdirs from 172.22.0.7:56742 Call#0 Retry#3: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot create directory /tmp/hadoop-root/yarn/system/rmstore/FSRMStateRoot/RMDTSecretManagerRoot. Name node is in safe mode.
The reported blocks 16 has reached the threshold 0.9990 of total blocks 16. The number of live datanodes 1 has reached the minimum number 0. In safe mode extension. Safe mode will be turned off automatically in 14 seconds.
2020-10-21 12:53:02,141 INFO org.apache.hadoop.ipc.Server: IPC Server handler 9 on 9000, call org.apache.hadoop.hdfs.protocol.ClientProtocol.mkdirs from 172.22.0.7:56742 Call#0 Retry#4: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot create directory /tmp/hadoop-root/yarn/system/rmstore/FSRMStateRoot/RMDTSecretManagerRoot. Name node is in safe mode.
The reported blocks 16 has reached the threshold 0.9990 of total blocks 16. The number of live datanodes 1 has reached the minimum number 0. In safe mode extension. Safe mode will be turned off automatically in 12 seconds.
2020-10-21 12:53:04,873 INFO org.apache.hadoop.ipc.Server: IPC Server handler 4 on 9000, call org.apache.hadoop.hdfs.protocol.ClientProtocol.mkdirs from 172.22.0.7:56742 Call#0 Retry#5: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot create directory /tmp/hadoop-root/yarn/system/rmstore/FSRMStateRoot/RMDTSecretManagerRoot. Name node is in safe mode.
The reported blocks 16 has reached the threshold 0.9990 of total blocks 16. The number of live datanodes 1 has reached the minimum number 0. In safe mode extension. Safe mode will be turned off automatically in 10 seconds.
2020-10-21 12:53:05,037 INFO org.apache.hadoop.hdfs.StateChange: STATE* Safe mode ON, in safe mode extension. 
The reported blocks 16 has reached the threshold 0.9990 of total blocks 16. The number of live datanodes 1 has reached the minimum number 0. In safe mode extension. Safe mode will be turned off automatically in 9 seconds.
2020-10-21 12:53:07,095 INFO org.apache.hadoop.ipc.Server: IPC Server handler 9 on 9000, call org.apache.hadoop.hdfs.protocol.ClientProtocol.mkdirs from 172.22.0.7:56742 Call#0 Retry#6: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot create directory /tmp/hadoop-root/yarn/system/rmstore/FSRMStateRoot/RMDTSecretManagerRoot. Name node is in safe mode.
The reported blocks 16 has reached the threshold 0.9990 of total blocks 16. The number of live datanodes 1 has reached the minimum number 0. In safe mode extension. Safe mode will be turned off automatically in 7 seconds.
2020-10-21 12:53:09,206 INFO org.apache.hadoop.ipc.Server: IPC Server handler 6 on 9000, call org.apache.hadoop.hdfs.protocol.ClientProtocol.mkdirs from 172.22.0.7:56742 Call#0 Retry#7: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot create directory /tmp/hadoop-root/yarn/system/rmstore/FSRMStateRoot/RMDTSecretManagerRoot. Name node is in safe mode.
The reported blocks 16 has reached the threshold 0.9990 of total blocks 16. The number of live datanodes 1 has reached the minimum number 0. In safe mode extension. Safe mode will be turned off automatically in 5 seconds.
2020-10-21 12:53:10,809 INFO org.apache.hadoop.ipc.Server: IPC Server handler 4 on 9000, call org.apache.hadoop.hdfs.protocol.ClientProtocol.mkdirs from 172.22.0.7:56742 Call#0 Retry#8: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot create directory /tmp/hadoop-root/yarn/system/rmstore/FSRMStateRoot/RMDTSecretManagerRoot. Name node is in safe mode.
The reported blocks 16 has reached the threshold 0.9990 of total blocks 16. The number of live datanodes 1 has reached the minimum number 0. In safe mode extension. Safe mode will be turned off automatically in 4 seconds.
2020-10-21 12:53:13,671 INFO org.apache.hadoop.ipc.Server: IPC Server handler 1 on 9000, call org.apache.hadoop.hdfs.protocol.ClientProtocol.mkdirs from 172.22.0.7:56742 Call#0 Retry#9: org.apache.hadoop.hdfs.server.namenode.SafeModeException: Cannot create directory /tmp/hadoop-root/yarn/system/rmstore/FSRMStateRoot/RMDTSecretManagerRoot. Name node is in safe mode.
The reported blocks 16 has reached the threshold 0.9990 of total blocks 16. The number of live datanodes 1 has reached the minimum number 0. In safe mode extension. Safe mode will be turned off automatically in 1 seconds.
2020-10-21 12:53:15,329 INFO org.apache.hadoop.hdfs.StateChange: STATE* Leaving safe mode after 35 secs
2020-10-21 12:53:15,392 INFO org.apache.hadoop.hdfs.StateChange: STATE* Safe mode is OFF
2020-10-21 12:53:15,399 INFO org.apache.hadoop.hdfs.StateChange: STATE* Network topology has 1 racks and 1 datanodes
2020-10-21 12:53:15,403 INFO org.apache.hadoop.hdfs.StateChange: STATE* UnderReplicatedBlocks has 0 blocks
2020-10-21 12:53:16,752 INFO org.apache.hadoop.hdfs.server.namenode.EditLogFileOutputStream: Nothing to flush
2020-10-21 12:53:16,819 INFO org.apache.hadoop.hdfs.server.namenode.EditLogFileOutputStream: Nothing to flush
2020-10-21 12:53:16,821 INFO org.apache.hadoop.hdfs.server.namenode.EditLogFileOutputStream: Nothing to flush
2020-10-21 12:53:17,462 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741843_1019{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-f81e3095-598a-4077-8723-10720198009d:NORMAL:172.22.0.7:50010|RBW]]} for /tmp/hadoop-root/yarn/system/rmstore/FSRMStateRoot/EpochNode.new.tmp
2020-10-21 12:53:17,802 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741843_1019{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-f81e3095-598a-4077-8723-10720198009d:NORMAL:172.22.0.7:50010|RBW]]} is not COMPLETE (ucState = COMMITTED, replication# = 0 <  minimum = 1) in file /tmp/hadoop-root/yarn/system/rmstore/FSRMStateRoot/EpochNode.new.tmp
2020-10-21 12:53:17,802 INFO org.apache.hadoop.hdfs.server.namenode.EditLogFileOutputStream: Nothing to flush
2020-10-21 12:53:17,878 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.22.0.7:50010 is added to blk_1073741843_1019{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-f81e3095-598a-4077-8723-10720198009d:NORMAL:172.22.0.7:50010|RBW]]} size 2
2020-10-21 12:53:18,225 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-root/yarn/system/rmstore/FSRMStateRoot/EpochNode.new.tmp is closed by DFSClient_NONMAPREDUCE_-935947980_1
2020-10-21 12:53:18,286 INFO BlockStateChange: BLOCK* addToInvalidates: blk_1073741830_1006 172.22.0.7:50010 
2020-10-21 12:53:18,528 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741844_1020{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-f81e3095-598a-4077-8723-10720198009d:NORMAL:172.22.0.7:50010|RBW]]} for /tmp/hadoop-root/yarn/system/rmstore/FSRMStateRoot/RMDTSecretManagerRoot/DelegationKey_5.tmp
2020-10-21 12:53:18,644 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.22.0.7:50010 is added to blk_1073741844_1020{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-f81e3095-598a-4077-8723-10720198009d:NORMAL:172.22.0.7:50010|RBW]]} size 0
2020-10-21 12:53:18,647 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-root/yarn/system/rmstore/FSRMStateRoot/RMDTSecretManagerRoot/DelegationKey_5.tmp is closed by DFSClient_NONMAPREDUCE_-935947980_1
2020-10-21 12:53:18,777 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741845_1021{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-f81e3095-598a-4077-8723-10720198009d:NORMAL:172.22.0.7:50010|RBW]]} for /tmp/hadoop-root/yarn/system/rmstore/FSRMStateRoot/RMDTSecretManagerRoot/DelegationKey_6.tmp
2020-10-21 12:53:18,884 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741845_1021{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-f81e3095-598a-4077-8723-10720198009d:NORMAL:172.22.0.7:50010|RBW]]} is not COMPLETE (ucState = COMMITTED, replication# = 0 <  minimum = 1) in file /tmp/hadoop-root/yarn/system/rmstore/FSRMStateRoot/RMDTSecretManagerRoot/DelegationKey_6.tmp
2020-10-21 12:53:18,886 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.22.0.7:50010 is added to blk_1073741845_1021{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-f81e3095-598a-4077-8723-10720198009d:NORMAL:172.22.0.7:50010|RBW]]} size 17
2020-10-21 12:53:19,294 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /tmp/hadoop-root/yarn/system/rmstore/FSRMStateRoot/RMDTSecretManagerRoot/DelegationKey_6.tmp is closed by DFSClient_NONMAPREDUCE_-935947980_1
2020-10-21 12:53:20,233 INFO BlockStateChange: BLOCK* BlockManager: ask 172.22.0.7:50010 to delete [blk_1073741830_1006]
2020-10-21 12:53:32,284 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741846_1022{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-f81e3095-598a-4077-8723-10720198009d:NORMAL:172.22.0.7:50010|RBW]]} for /data/flume/flume.log-2020-10-21-12-53-24.1603284804357.tmp
2020-10-21 12:53:36,837 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* fsync: /data/flume/flume.log-2020-10-21-12-53-24.1603284804357.tmp for DFSClient_NONMAPREDUCE_1354015655_28
2020-10-21 12:53:36,859 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741847_1023{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-f81e3095-598a-4077-8723-10720198009d:NORMAL:172.22.0.7:50010|RBW]]} for /data/flume/flume.log-2020-10-21-12-53-28.1603284808803.tmp
2020-10-21 12:53:36,926 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* fsync: /data/flume/flume.log-2020-10-21-12-53-28.1603284808803.tmp for DFSClient_NONMAPREDUCE_1354015655_28
2020-10-21 12:53:36,950 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741848_1024{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-f81e3095-598a-4077-8723-10720198009d:NORMAL:172.22.0.7:50010|RBW]]} for /data/flume/flume.log-2020-10-21-12-53-29.1603284809004.tmp
2020-10-21 12:53:37,294 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* fsync: /data/flume/flume.log-2020-10-21-12-53-29.1603284809004.tmp for DFSClient_NONMAPREDUCE_1354015655_28
2020-10-21 12:53:48,855 INFO org.apache.hadoop.hdfs.server.namenode.FSEditLog: Number of transactions: 39 Total time for transactions(ms): 18 Number of transactions batched in Syncs: 1 Number of syncs: 31 SyncTimes(ms): 36 
2020-10-21 12:53:48,949 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* allocate blk_1073741849_1025{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-f81e3095-598a-4077-8723-10720198009d:NORMAL:172.22.0.7:50010|RBW]]} for /data/flume/flume.log-2020-10-21-12-53-48.1603284828783.tmp
2020-10-21 12:53:48,972 INFO org.apache.hadoop.hdfs.StateChange: BLOCK* fsync: /data/flume/flume.log-2020-10-21-12-53-48.1603284828783.tmp for DFSClient_NONMAPREDUCE_1354015655_28
2020-10-21 12:53:58,803 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.22.0.7:50010 is added to blk_1073741846_1022{UCState=UNDER_CONSTRUCTION, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-f81e3095-598a-4077-8723-10720198009d:NORMAL:172.22.0.7:50010|RBW]]} size 61
2020-10-21 12:53:58,825 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /data/flume/flume.log-2020-10-21-12-53-24.1603284804357.tmp is closed by DFSClient_NONMAPREDUCE_1354015655_28
2020-10-21 12:53:59,023 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741847_1023{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-f81e3095-598a-4077-8723-10720198009d:NORMAL:172.22.0.7:50010|RBW]]} is not COMPLETE (ucState = COMMITTED, replication# = 0 <  minimum = 1) in file /data/flume/flume.log-2020-10-21-12-53-28.1603284808803.tmp
2020-10-21 12:53:59,023 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.22.0.7:50010 is added to blk_1073741847_1023{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-f81e3095-598a-4077-8723-10720198009d:NORMAL:172.22.0.7:50010|RBW]]} size 109
2020-10-21 12:53:59,427 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /data/flume/flume.log-2020-10-21-12-53-28.1603284808803.tmp is closed by DFSClient_NONMAPREDUCE_1354015655_28
2020-10-21 12:53:59,453 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem: BLOCK* blk_1073741848_1024{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-f81e3095-598a-4077-8723-10720198009d:NORMAL:172.22.0.7:50010|RBW]]} is not COMPLETE (ucState = COMMITTED, replication# = 0 <  minimum = 1) in file /data/flume/flume.log-2020-10-21-12-53-29.1603284809004.tmp
2020-10-21 12:53:59,457 INFO BlockStateChange: BLOCK* addStoredBlock: blockMap updated: 172.22.0.7:50010 is added to blk_1073741848_1024{UCState=COMMITTED, truncateBlock=null, primaryNodeIndex=-1, replicas=[ReplicaUC[[DISK]DS-f81e3095-598a-4077-8723-10720198009d:NORMAL:172.22.0.7:50010|RBW]]} size 400
2020-10-21 12:53:59,857 INFO org.apache.hadoop.hdfs.StateChange: DIR* completeFile: /data/flume/flume.log-2020-10-21-12-53-29.1603284809004.tmp is closed by DFSClient_NONMAPREDUCE_1354015655_28
